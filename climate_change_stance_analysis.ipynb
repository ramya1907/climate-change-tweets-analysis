{"cells":[{"cell_type":"markdown","source":["# Analysing Climate Change Discourse on Twitter"],"metadata":{"id":"ppQeaOKPirot"}},{"cell_type":"markdown","source":["# Run Analysis"],"metadata":{"id":"ImjrvBUVv1R3"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"5yNNl1c-iz3R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712413233482,"user_tz":-240,"elapsed":3167,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}},"outputId":"68ba9283-7159-40c7-bce6-d0682d9d9f6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/myfiles; to attempt to forcibly remount, call drive.mount(\"/content/myfiles\", force_remount=True).\n","/content/myfiles/My Drive/Colab_Notebooks\n","/content/myfiles/MyDrive/Colab_Notebooks\n","origin\thttps://git-colab:glpat-ro8so_TtrHYLTq32YHQb@git.cs.bham.ac.uk/projects-2023-24/rxs008.git (fetch)\n","origin\thttps://git-colab:glpat-ro8so_TtrHYLTq32YHQb@git.cs.bham.ac.uk/projects-2023-24/rxs008.git (push)\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/myfiles')\n","\n","!pwd\n","\n","%cd \"/content/myfiles/MyDrive/Colab_Notebooks/\"\n","\n","!git remote -v"]},{"cell_type":"markdown","metadata":{"id":"ennzsx_bJw5R"},"source":["## Imports and Installations"]},{"cell_type":"code","source":["!pip install symspellpy\n","!pip install contractions"],"metadata":{"id":"xRnF_PVAuLWP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712413246079,"user_tz":-240,"elapsed":12601,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}},"outputId":"076b411f-9699-430d-db92-f84792c2197f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: symspellpy in /usr/local/lib/python3.10/dist-packages (6.7.7)\n","Requirement already satisfied: editdistpy>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from symspellpy) (0.1.3)\n","Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n","Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n","Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n","Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YL-lyixHzoSb"},"outputs":[],"source":["import chardet\n","from collections import Counter\n","import contractions\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import re\n","import unicodedata\n","import string\n","import nltk\n","import en_core_web_sm\n","import seaborn as sns\n","import spacy\n","from symspellpy import SymSpell, Verbosity\n","import sys\n","from wordcloud import WordCloud\n","sys.setrecursionlimit(10000)\n","\n","nlp = spacy.load('en_core_web_sm')\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger') #pos_tag\n","from nltk.corpus import stopwords, wordnet\n","from nltk.stem import WordNetLemmatizer\n","from nltk.probability import FreqDist\n","from nltk.tokenize import word_tokenize\n","\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import LinearSVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import confusion_matrix\n","from sklearn import metrics\n","from sklearn.metrics import accuracy_score, classification_report, f1_score, silhouette_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import AgglomerativeClustering\n","\n","from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n","\n","import networkx as nx\n","import plotly.graph_objects as go\n"]},{"cell_type":"markdown","metadata":{"id":"x4v575KTaJD6"},"source":["##Data Cleaning"]},{"cell_type":"code","source":["data_world_file_path = os.path.join(\"Dataset\", \"data_world.csv\")\n","kaggle_file_path = os.path.join(\"Dataset\", \"kaggle.csv\")\n","kaggle_2020_file_path = os.path.join(\"Dataset\", \"kaggle_2020.csv\")"],"metadata":{"id":"qxO0TD6586Na"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Oa1IEPrnKyf"},"outputs":[],"source":["# Detect encoding\n","with open(data_world_file_path, 'rb') as f:\n","  result = chardet.detect(f.read())\n","  encoding = result['encoding']\n","\n","'''\n","  Initially, data_world.csv had 6090 rows.\n","  After skipping on bad lines, there are 6087 rows in this dataset\n","  Finally, after dropping duplicates, the number of rows is now 5541.\n","'''\n","\n","data_world_df = pd.read_csv(data_world_file_path, encoding=encoding, on_bad_lines='skip')\n","\n","# Remove duplicate tweets\n","data_world_df = data_world_df.drop_duplicates(subset='tweet', keep='first')\n","\n","# Change representation of 'existence' values\n","data_world_df['existence'] = data_world_df['existence'].map({'Yes': 1, 'No': -1, 'Neutral': 0})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mO3yR7Exxtm6"},"outputs":[],"source":["# Repeat same steps as above for Kaggle dataset\n","\n","with open(kaggle_file_path, 'rb') as f:\n","  result = chardet.detect(f.read())\n","  encoding = result['encoding']\n","\n","'''\n","  Initially, kaggle.csv had 43,943 rows.\n","  After dropping duplicates, the number of rows reduced to 41,033\n","  Finally, after removing rows labelled as \"News\", the dataset now has 31,960 rows\n","'''\n","\n","kaggle_df = pd.read_csv(kaggle_file_path, encoding=encoding, on_bad_lines='skip')\n","kaggle_df = kaggle_df.drop_duplicates(subset='message', keep='first')\n","\n","# Remove rows where stance is labeled as News (value = 2)\n","kaggle_df = kaggle_df.loc[kaggle_df['sentiment'] != 2]"]},{"cell_type":"code","source":["# Repeat same steps as above for Kaggle_2020 dataset\n","with open(kaggle_2020_file_path, 'rb') as f:\n","  result = chardet.detect(f.read())\n","  encoding = result['encoding']\n","\n","# Create DataFrame for test data (396 rows)\n","test_df = pd.read_csv(kaggle_2020_file_path, encoding=encoding, on_bad_lines='skip')\n","test_df = test_df.drop_duplicates(subset='text', keep='first')\n","test_df.drop(columns=['Unnamed: 1', 'Unnamed: 2'], inplace=True)\n","test_df.rename(columns={'text': 'tweet'}, inplace=True)"],"metadata":{"id":"YgfLseenLCXC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JC1R_j2s1ZQz"},"outputs":[],"source":["# Rename columns of kaggle_df and data_world_df to the same\n","df_selected_columns = data_world_df[['tweet', 'existence']].rename(columns={'existence': 'label'})\n","df_kaggle_selected_columns = kaggle_df[['message', 'sentiment']].rename(columns={'message': 'tweet', 'sentiment': 'label'})\n","\n","'''\n","Initializing a dataframe to store tweets awaiting classification labels\n","based on the selected model.\n","'''\n","\n","# Concatenate the two DataFrames vertically\n","df = pd.concat([df_selected_columns, df_kaggle_selected_columns], ignore_index=True)"]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"xw3o3j0_u6WU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df.head()"],"metadata":{"id":"HUoN6DdOu8bB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jPHBm-B4cQ-z"},"source":["##Data Pre-Processing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oX-acGRVWWe1"},"outputs":[],"source":["# Function to fix encoding issues\n","def fix_encoding(text):\n","  try:\n","    decoded_text = text.encode('utf-8').decode('utf-8')\n","  except Exception as e:  # Catch other potential errors\n","    decoded_text = ''  # Return empty string for other errors\n","  return decoded_text\n","\n","def remove_punctuation(text):\n","  cleaned_text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n","  return cleaned_text\n","\n","def reduce_lengthening(text):\n","  pattern = re.compile(r\"(.)\\1{2,}\")\n","  return pattern.sub(r\"\\1\\1\", text)\n","\n","# Initialize SymSpell\n","sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n","dictionary_path = \"frequency_dictionary_en_82_765.txt\"\n","sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n","\n","def fix_spelling(tweet):\n","  # Perform spell checking and correction\n","  result = []\n","  for word in tweet.split():\n","    suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2, include_unknown=True)\n","    corrected_word = suggestions[0].term if suggestions else word\n","    result.append(corrected_word)\n","  corrected_tweet = ' '.join(result)\n","  return corrected_tweet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dxqwgZHocQYT"},"outputs":[],"source":["# Method for Pre-processing\n","def clean_tweet(tweet):\n","\n","  # Remove retweet texts \"RT\"\n","  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","\n","  # Converting to lower case\n","  tweet = tweet.lower()\n","\n","  # Remove URLs\n","  tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n","\n","  # Remove placeholders saying [link] which was done by prior text cleaning\n","  tweet = re.sub(r'\\[link\\]', '', tweet)\n","\n","  # Remove numbers\n","  tweet = re.sub(r'\\d+', '', tweet)\n","\n","  # Remove mentions\n","  tweet = re.sub('@[\\w]*', '', tweet)\n","\n","  # Expand contractions\n","  tweet = contractions.fix(tweet)\n","\n","  # Remove punctuation\n","  tweet = remove_punctuation(tweet)\n","\n","  # Remove diamond symbol\n","  tweet = re.sub(r\"U+FFFD \", '', tweet)\n","\n","  # Reduce lengthening\n","  tweet = reduce_lengthening(tweet)\n","\n","  # Correct spelling\n","  tweet = fix_spelling(tweet)\n","\n","  # Remove extra whitespace\n","  tweet = re.sub(r'\\s\\s+', '', tweet)\n","\n","  # Remove leading spaces\n","  tweet = tweet.lstrip(' ')\n","\n","  tweet = unicodedata.normalize('NFKD', tweet).encode('ascii','ignore').decode('utf-8', 'ignore')\n","\n","  # Fix encoding issues\n","  tweet = fix_encoding(tweet)\n","\n","  return tweet\n"]},{"cell_type":"code","source":["df['cleaned_tweet'] = df['tweet'].apply(clean_tweet)\n","test_df['cleaned_tweet'] = test_df['tweet'].apply(clean_tweet)"],"metadata":{"id":"yqw-71tB73AE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBjkXtcR-6Yn"},"outputs":[],"source":["# Remove duplicate tweets after pre-processing\n","df = df.drop_duplicates(subset='cleaned_tweet', keep='first')\n","test_df= test_df.drop_duplicates(subset='cleaned_tweet', keep='first')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zjXXrOWIe6rQ"},"outputs":[],"source":["# Tokenizing\n","df['tokenized_tweet'] = df['cleaned_tweet'].apply(lambda x: nlp(x))\n","test_df['tokenized_tweet'] = test_df['cleaned_tweet'].apply(lambda x: nlp(x))\n","\n","stop_words = stopwords.words('english')\n","stop_words += list(string.punctuation) # !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n","stop_words += list(string.ascii_lowercase) # letters from 'a' to 'z'\n","\n","def clean_tokens(tokens):\n","  cleaned_tokens = [word.text for word in tokens if word.text not in set(stop_words)]\n","  return cleaned_tokens\n","\n","df['tokenized_tweet'] = df['tokenized_tweet'].apply(clean_tokens)\n","test_df['tokenized_tweet'] = test_df['tokenized_tweet'].apply(clean_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"psWtjx5EoZ7H"},"outputs":[],"source":["# Removing rare words from dataset to reduce bias, threshold = 2\n","all_tweets = [' '.join(tokens) for tokens in df['tokenized_tweet']]\n","\n","def find_rare_words(threshold=2, all_text=all_tweets, top_n_rare_words=10):\n","  words = nltk.word_tokenize(' '.join(all_text))\n","  word_freq = Counter(words)\n","\n","  rare_words = [word for word, freq in word_freq.items() if freq < threshold][:top_n_rare_words]\n","\n","  return rare_words\n","\n","def remove_rare_words(text, rare_words):\n","  filtered_words = [word for word in text if word not in rare_words]\n","  return filtered_words\n","\n","rare_words_list = find_rare_words()\n","df['tokenized_tweet'] = df['tokenized_tweet'].apply(remove_rare_words, rare_words=rare_words_list)\n","test_df['tokenized_tweet'] = test_df['tokenized_tweet'].apply(remove_rare_words, rare_words=rare_words_list)"]},{"cell_type":"markdown","source":["### Lemmatization"],"metadata":{"id":"DkJYSSKNe-ug"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"G-3bguL-xqcz"},"outputs":[],"source":["# Lemmatizing\n","def lemma(df):\n","\n","  # Add part-of-speech tags to the 'tweet' column\n","  df['pos_tags'] = df['tokenized_tweet'].apply(nltk.tag.pos_tag)\n","\n","  def get_wordnet_pos(tweet_tag):\n","\n","    # Map Penn Treebank POS tags to WordNet POS tags.\n","    pos_mapping = {\n","      'J': wordnet.ADJ,\n","      'V': wordnet.VERB,\n","      'N': wordnet.NOUN,\n","      'R': wordnet.ADV\n","    }\n","    return wordnet.NOUN\n","\n","  lemmatizer = WordNetLemmatizer()\n","\n","  # Lemmatize the 'tweet' column based on part-of-speech tags\n","  df['lemma'] = df['pos_tags'].apply(lambda x: [lemmatizer.lemmatize(word, get_wordnet_pos(pos_tag)) for word, pos_tag in x])\n","\n","  # ['apple', 'banana', 'orange'] -> 'apple banana orange'\n","  df['lemma'] = [' '.join(map(str, l)) for l in df['lemma']]\n","\n","  df.drop('pos_tags', axis=1, inplace=True)\n","\n","  return df"]},{"cell_type":"code","source":["df = lemma(df)\n","test_df = lemma(test_df)"],"metadata":{"id":"IHtBrUlrbdau"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l90eUXp9r9Bn"},"source":["## Data Visualisations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g7vMnE6ODhPb"},"outputs":[],"source":["# Finding the 35 most commonly mentioned words for each stance label\n","def calculate_word_frequency(text_corpus):\n","\n","  tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n","  word_matrix = tfidf_vectorizer.fit_transform(text_corpus)\n","  feature_names = tfidf_vectorizer.get_feature_names_out()\n","\n","  tfidf_df = pd.DataFrame(word_matrix.toarray(), columns=feature_names)\n","  top_words = tfidf_df.mean().sort_values(ascending=False).head(10)\n","\n","  # Get mean TF-IDF score for each word across all documents\n","  mean_tfidf_scores = tfidf_df.mean()\n","\n","  # Sort words based on their mean TF-IDF scores in descending order\n","  top_words = mean_tfidf_scores.sort_values(ascending=False).head(35)\n","  return top_words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIRSh5KtPr92"},"outputs":[],"source":["# Retrieving the top 35 words in each class\n","pro_top_35 = calculate_word_frequency(df['lemma'][df['label']==1])\n","anti_top_35 = calculate_word_frequency(df['lemma'][df['label']==-1])\n","neutral_top_35 = calculate_word_frequency(df['lemma'][df['label']==0])\n","\n","# Turning the above variables into lists to use as data for wordclouds\n","pro_list = ' '.join(pro_top_35.index)\n","anti_list = ' '.join(anti_top_35.index)\n","neutral_list = ' '.join(neutral_top_35.index)\n","\n","# Generating wordclouds\n","anti_wc = WordCloud(background_color='white', colormap='gist_heat', width = 800, height = 500).generate(anti_list)\n","pro_wc = WordCloud(background_color='white', colormap='Greens', width = 800, height = 500).generate(pro_list)\n","neutral_wc = WordCloud(background_color='white', colormap='PuBuGn_r', width = 800, height = 500).generate(neutral_list)\n","\n","fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","# Plot Anti Word Cloud\n","axes[0].imshow(anti_wc, interpolation='bilinear')\n","axes[0].set_title('Anti Word Cloud')\n","axes[0].axis('off')\n","\n","# Plot Pro Word Cloud\n","axes[1].imshow(pro_wc, interpolation='bilinear')\n","axes[1].set_title('Pro Word Cloud')\n","axes[1].axis('off')\n","\n","# Plot Neutral Word Cloud\n","axes[2].imshow(neutral_wc, interpolation='bilinear')\n","axes[2].set_title('Neutral Word Cloud')\n","axes[2].axis('off')\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"YGguyO5x2aAR"},"source":["## Stance Classification"]},{"cell_type":"markdown","source":["### Fitting Classification Models"],"metadata":{"id":"OLev5TPMe9Go"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCMPIR62MO68"},"outputs":[],"source":["df.dropna(inplace=True)\n","\n","x = df['lemma']\n","y = df['label']\n","\n","# Split the train data to create validation dataset\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=123)"]},{"cell_type":"code","source":["#Creating pipelines for each classifier\n","\n","# Random Forest\n","rf_clf = Pipeline([\n","  ('tfidf', TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_df=0.95)),\n","  ('rf', RandomForestClassifier(max_depth=20, n_estimators=100))\n","])\n","\n","# Logistic Regression\n","lr_clf = Pipeline([\n","  ('tfidf', TfidfVectorizer(ngram_range=(1, 3), min_df=5, max_df=0.85)),\n","  ('lr', LogisticRegression(C=0.5, class_weight='balanced', max_iter=2000))\n","])\n","\n","# Linear SVC:\n","lsvc_clf = Pipeline([\n","  ('tfidf', TfidfVectorizer(ngram_range=(1, 2), min_df=2, max_df=0.85)),\n","  ('lsvc', LinearSVC(class_weight='balanced', C=0.1))\n","])\n","\n","# Naïve Bayes:\n","nb_clf = Pipeline([\n","  ('tfidf', TfidfVectorizer(ngram_range=(1, 2), min_df=5, max_df=0.85)),\n","  ('nb', MultinomialNB(alpha=0.5))\n","])\n","\n","# K-NN Classifier\n","knn_clf = Pipeline([\n","  ('tfidf', TfidfVectorizer(ngram_range=(1, 2), min_df=4, max_df=0.8)),\n","  ('knn', KNeighborsClassifier(n_neighbors=5, metric='cosine'))\n","])"],"metadata":{"id":"dauIVz45TeTF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPJAvVIAMj1O"},"outputs":[],"source":["# Fitting models\n","\n","# Random Forest\n","rf_clf.fit(x_train, y_train)\n","rf_pred = rf_clf.predict(x_test)\n","\n","# Logistic Regression\n","lr_clf.fit(x_train, y_train)\n","lr_pred = lr_clf.predict(x_test)\n","\n","# Linear SVC\n","lsvc_clf.fit(x_train, y_train)\n","lsvc_pred = lsvc_clf.predict(x_test)\n","\n","# Naive Bayes\n","nb_clf.fit(x_train, y_train)\n","nb_pred = nb_clf.predict(x_test)\n","\n","# K-Nearest Neighbors\n","knn_clf.fit(x_train, y_train)\n","knn_pred = knn_clf.predict(x_test)"]},{"cell_type":"markdown","metadata":{"id":"bkIIwlVB1nT3"},"source":["### Classification Model Reports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"trS_OMG1SiuD"},"outputs":[],"source":["# Classification Report for the Random Forest model\n","print(metrics.classification_report(y_test, rf_pred, zero_division=1))\n","\n","conf_matrix = confusion_matrix(y_test, rf_pred)\n","conf_matrix_norm = conf_matrix / conf_matrix.sum(axis=1).reshape(-1,1)\n","\n","# Confusion matrix\n","sns.heatmap(conf_matrix_norm, annot=True, cmap='Blues',  xticklabels=rf_clf.classes_,\n","            yticklabels=rf_clf.classes_,\n","            vmin=0.,\n","            vmax=1.,\n","            annot_kws={'size':10})\n","\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix for Random Forest Model')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzRiaE7CMxBm"},"outputs":[],"source":["# Classification Report for the Logistic Regression model\n","print(metrics.classification_report(y_test, lr_pred))\n","\n","conf_matrix = confusion_matrix(y_test, lr_pred)\n","conf_matrix_norm = conf_matrix / conf_matrix.sum(axis=1).reshape(-1,1)\n","\n","# Confusion matrix\n","sns.heatmap(conf_matrix_norm,\n","            annot=True,\n","            cmap='Blues',\n","            xticklabels=lr_clf.classes_,\n","            yticklabels=lr_clf.classes_,\n","            vmin=0.,\n","            vmax=1.,\n","            annot_kws={'size':10})\n","\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix for Logistic Regression Model')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KeT7IyE_g-9M"},"outputs":[],"source":["# Classification Report for the Linear SVC model\n","print(metrics.classification_report(y_test, lsvc_pred))\n","\n","conf_matrix = confusion_matrix(y_test, lsvc_pred)\n","conf_matrix_norm = conf_matrix / conf_matrix.sum(axis=1).reshape(-1,1)\n","\n","# Confusion matrix\n","sns.heatmap(conf_matrix_norm,\n","            annot=True,\n","            cmap='Blues',\n","            xticklabels=lsvc_clf.classes_,\n","            yticklabels=lsvc_clf.classes_,\n","            vmin=0.,\n","            vmax=1.,\n","            annot_kws={'size':10})\n","\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix for Linear SVC Model')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0q2AOPtOOAA2"},"outputs":[],"source":["# Classification Report for the Naive Bayes model\n","print(metrics.classification_report(y_test, nb_pred))\n","\n","conf_matrix = confusion_matrix(y_test, nb_pred)\n","conf_matrix_norm = conf_matrix / conf_matrix.sum(axis=1).reshape(-1,1)\n","\n","# Confusion matrix\n","sns.heatmap(conf_matrix_norm,\n","            annot=True,\n","            cmap='Blues',\n","            xticklabels=nb_clf.classes_,\n","            yticklabels=nb_clf.classes_,\n","            vmin=0.,\n","            vmax=1.,\n","            annot_kws={'size':10})\n","\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix for Naive Bayes Model')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DFxslly_SJzn"},"outputs":[],"source":["# Classification Report for the K-NN model\n","print(metrics.classification_report(y_test, knn_pred))\n","\n","conf_matrix = confusion_matrix(y_test, knn_pred)\n","conf_matrix_norm = conf_matrix / conf_matrix.sum(axis=1).reshape(-1,1)\n","\n","# Confusion matrix\n","sns.heatmap(conf_matrix_norm,\n","            annot=True,\n","            cmap='Blues',\n","            xticklabels=knn_clf.classes_,\n","            yticklabels=knn_clf.classes_,\n","            vmin=0.,\n","            vmax=1.,\n","            annot_kws={'size':10})\n","\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.title('Confusion Matrix for K-Nearest Neighbours Model')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"FdDt_b82Wae7"},"source":["### Model Prediction on Unlabelled Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1MFUI_D_wZIr"},"outputs":[],"source":["test_df.dropna(inplace=True)\n","pred_data = test_df['lemma']\n","lsvc_pred = lsvc_clf.predict(pred_data)\n","\n","# Add predicted label to 'test_df'\n","test_df['predicted_label'] = lsvc_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QBW89NCCx97m"},"outputs":[],"source":["# Combine test_df and df\n","test_df.rename(columns={'predicted_label': 'label'}, inplace=True)\n","combined_df = pd.concat([df, test_df], ignore_index=True)"]},{"cell_type":"code","source":["\"\"\"\n","  CSV file created so that pre-processing and stance classification\n","  of data does not need to be run every time for subsequent steps.\n","\"\"\"\n","combined_df.to_csv('combined_data.csv', index=False, encoding='utf-8')"],"metadata":{"id":"cb7Rg92_i2hj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Clustering"],"metadata":{"id":"ZERINy0BHDnt"}},{"cell_type":"markdown","source":["### K-Means Clustering"],"metadata":{"id":"z7ZAXrp9QGNE"}},{"cell_type":"code","source":["# Load data from CSV file\n","combined_df = pd.read_csv('combined_data.csv')\n","combined_df.dropna(inplace=True)"],"metadata":{"id":"1tR1j_ixd1qf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Clustering Using Pro, Anti and Neutral Tweets"],"metadata":{"id":"CV9IAs-u9nAI"}},{"cell_type":"code","source":["# Create corpus\n","corpus = combined_df['tokenized_tweet'].tolist()\n","\n","tfidf_vectorizer = TfidfVectorizer(min_df=8, max_df=0.85)\n","\n","# Apply tfidf embedding\n","X = tfidf_vectorizer.fit_transform(corpus)\n","vocab = tfidf_vectorizer.get_feature_names_out()\n","\n","kmeans = KMeans(n_clusters=5, random_state=13, n_init='auto')\n","clusters = kmeans.fit_predict(X)\n","centroids = kmeans.cluster_centers_\n","\n","# Prinicipal Component Analysis\n","pca = PCA(n_components=2, random_state=1048)\n","X_pca = pca.fit_transform(X.toarray())\n","\n","# Calculate the percentage of pro, anti, and neutral tweets in each cluster\n","cluster_stance_counts = {}\n","\n","for cluster_id in range(kmeans.n_clusters):\n","  cluster_indices = np.where(clusters == cluster_id)[0]\n","  cluster_stances = combined_df.iloc[cluster_indices]['label']\n","  pro_count = (cluster_stances == 1).sum()\n","  anti_count = (cluster_stances == -1).sum()\n","  neutral_count = (cluster_stances == 0).sum()\n","  total_count = len(cluster_indices)\n","  cluster_stance_counts[f'Cluster {cluster_id + 1}'] = {\n","    'Pro': round(pro_count / total_count * 100, 2),\n","    'Anti': round(anti_count / total_count * 100, 2),\n","    'Neutral': round(neutral_count / total_count * 100, 2)\n","  }\n","\n","# Visualize the clusters\n","plt.figure(figsize=(10, 8))\n","plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', marker='o', alpha=0.5)\n","plt.title('PCA Visualization of Clusters: Pro, Anti and Neutral')\n","plt.xlabel('PCA Component 1')\n","plt.ylabel('PCA Component 2')\n","plt.colorbar(label='Cluster')\n","plt.show()\n","\n","n_top_terms = 10\n","top_term_indices = centroids.argsort()[:, ::-1][:, :n_top_terms]\n","\n","top_terms_per_cluster = {}\n","for cluster_id, indices in enumerate(top_term_indices):\n","  terms = [vocab[idx] for idx in indices]\n","  top_terms_per_cluster[f'Cluster {cluster_id + 1}'] = terms\n","\n","top_terms_df = pd.DataFrame.from_dict(top_terms_per_cluster, orient='index')\n","top_terms_df = top_terms_df.transpose()\n","\n","# Convert stance distribution dictionary to DataFrame\n","stance_distribution_df = pd.DataFrame.from_dict(cluster_stance_counts, orient='index')\n","\n","# Display both tables\n","print(\"Top Terms per Cluster:\")\n","print(top_terms_df)\n","print(\"\\nstance Distribution per Cluster:\")\n","print(stance_distribution_df)"],"metadata":{"id":"OtoGY53hJtlj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Elbow Plot\n","# For approx. 10% sample\n","\n","sample_indices = np.random.choice(X.shape[0], size=3500, replace=False)\n","X_sample = X[sample_indices, :]\n","\n","# Range of k to try\n","k_range = range(1, 101)\n","\n","# Calculate inertia (sum of squared distances) for each k\n","inertias = []\n","for k in k_range:\n","  kmeans = KMeans(n_clusters=k, random_state=1048, n_init='auto')\n","  kmeans.fit(X_sample)\n","  inertias.append(kmeans.inertia_)\n","\n","# Plot the elbow plot\n","plt.figure(figsize=(10, 8))\n","plt.plot(k_range, inertias, '-o')\n","plt.title('Elbow Method for Optimal k')\n","plt.xlabel('Number of clusters (k)')\n","plt.ylabel('Sum of squared distances (Inertia)')\n","plt.xticks(ticks=list(range(0, 101, 10)))\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"cggOS6JJkty-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Clustering Using Anti Tweets"],"metadata":{"id":"gAOKa5wy9spw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOEwcMB7p2F7"},"outputs":[],"source":["# Create Anti DataFrame\n","# anti_df 4756 rows\n","\n","anti_df = combined_df[combined_df['label'] == -1]\n","anti_corpus = anti_df['tokenized_tweet'].tolist()\n","\n","tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.9)\n","\n","# Apply tfidf embedding for anti stance\n","anti_X = tfidf_vectorizer.fit_transform(anti_corpus)\n","anti_vocab = tfidf_vectorizer.get_feature_names_out()\n","\n","anti_kmeans = KMeans(n_clusters=5, random_state=14, n_init='auto')\n","anti_clusters = anti_kmeans.fit_predict(anti_X)\n","anti_centroids = anti_kmeans.cluster_centers_\n","\n","n_top_terms = 10\n","top_term_indices = anti_centroids.argsort()[:, ::-1][:, :n_top_terms]\n","\n","top_terms_per_cluster_anti = {}\n","for cluster_id, indices in enumerate(top_term_indices):\n","  terms = [anti_vocab[idx] for idx in indices]\n","  top_terms_per_cluster_anti[f'Cluster {cluster_id + 1}'] = terms\n","\n","top_terms_df = pd.DataFrame.from_dict(top_terms_per_cluster_anti, orient='index')\n","top_terms_df = top_terms_df.transpose()\n","print(top_terms_df)\n","\n","pca = PCA(n_components=2, random_state=1048)\n","anti_X_pca = pca.fit_transform(anti_X.toarray())\n","\n","# Visualize the clusters\n","plt.figure(figsize=(10, 8))\n","plt.scatter(anti_X_pca[:, 0], anti_X_pca[:, 1], c=anti_clusters, cmap='viridis', marker='o', alpha=0.5)\n","plt.title('PCA Visualization of Clusters for Anti Stance')\n","plt.xlabel('PCA Component 1')\n","plt.ylabel('PCA Component 2')\n","plt.colorbar(label='Cluster')\n","\n","plt.show()\n","\n"]},{"cell_type":"code","source":["# Elbow Plot for Anti-Stance Dataset\n","\n","# Range of k to try\n","k_range = range(1, 101)\n","\n","# Calculate inertia (sum of squared distances) for each k\n","inertias = []\n","for k in k_range:\n","  kmeans = KMeans(n_clusters=k, random_state=1048, n_init='auto')\n","  kmeans.fit(anti_X)\n","  inertias.append(kmeans.inertia_)\n","\n","# Plot the elbow plot\n","plt.figure(figsize=(10, 8))\n","plt.plot(k_range, inertias, '-o')\n","plt.title('Elbow Method for Optimal k for Anti-Stance')\n","plt.xlabel('Number of clusters (k)')\n","plt.ylabel('Sum of squared distances (Inertia)')\n","plt.xticks(ticks=list(range(0, 101, 10)))\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"HCrlbXKx5DZa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Clustering Using Pro Tweets"],"metadata":{"id":"uy31iIZ291n4"}},{"cell_type":"code","source":["# Create Pro DataFrame\n","pro_df = combined_df[combined_df['label'] == 1]\n","pro_corpus = pro_df['tokenized_tweet'].tolist()\n","\n","tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.9)\n","pro_X = tfidf_vectorizer.fit_transform(pro_corpus)\n","pro_vocab = tfidf_vectorizer.get_feature_names_out()\n","\n","pro_kmeans = KMeans(n_clusters=5, random_state=24, n_init='auto')\n","pro_clusters = pro_kmeans.fit_predict(pro_X)\n","pro_centroids = pro_kmeans.cluster_centers_\n","\n","# Dimensionality reduction using PCA\n","pca = PCA(n_components=2, random_state=1048)\n","pro_X_pca = pca.fit_transform(pro_X.toarray())\n","\n","# Visualize the clusters\n","plt.figure(figsize=(10, 8))\n","plt.scatter(pro_X_pca[:, 0], pro_X_pca[:, 1], c=pro_clusters, cmap='viridis', marker='o', alpha=0.5)\n","plt.title('PCA Visualization of Clusters for Pro Stance')\n","plt.xlabel('PCA Component 1')\n","plt.ylabel('PCA Component 2')\n","plt.colorbar(label='Cluster')\n","plt.show()\n","\n","n_top_terms = 10\n","top_term_indices = pro_centroids.argsort()[:, ::-1][:, :n_top_terms]\n","\n","top_terms_per_cluster_pro = {}\n","for cluster_id, indices in enumerate(top_term_indices):\n","  terms = [pro_vocab[idx] for idx in indices]\n","  top_terms_per_cluster_pro[f'Cluster {cluster_id + 1}'] = terms\n","\n","top_terms_df = pd.DataFrame.from_dict(top_terms_per_cluster_pro, orient='index')\n","top_terms_df = top_terms_df.transpose()\n","print(top_terms_df)\n"],"metadata":{"id":"-12bSrX8ERhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Elbow Plot for Pro-Stance Dataset\n","sample_indices = np.random.choice(pro_X.shape[0], size=5000, replace=False)\n","pro_X_sample = pro_X[sample_indices, :]\n","\n","# Range of k to try\n","k_range = range(1, 101)\n","\n","# Calculate inertia (sum of squared distances) for each k\n","inertias = []\n","for k in k_range:\n","  kmeans = KMeans(n_clusters=k, random_state=1048, n_init='auto')\n","  kmeans.fit(pro_X_sample)\n","  inertias.append(kmeans.inertia_)\n","\n","# Plot the elbow plot\n","plt.figure(figsize=(10, 8))\n","plt.plot(k_range, inertias, '-o')\n","plt.title('Elbow Method for Optimal k for Pro-Stance')\n","plt.xlabel('Number of clusters (k)')\n","plt.ylabel('Sum of squared distances (Inertia)')\n","plt.xticks(ticks=list(range(0, 101, 10)))\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"EycGjtNz5-eh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VRM85uW4rqy2"},"source":["### Hierarchical Clustering"]},{"cell_type":"code","source":["# Load data from CSV file\n","combined_df = pd.read_csv('combined_data.csv')\n","combined_df.dropna(inplace=True)"],"metadata":{"id":"19dOZC_lVKo3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Clustering Using Pro, Anti and Neutral Stance Tweets"],"metadata":{"id":"ZIPO_M-tthoM"}},{"cell_type":"code","source":["tfidf_vectorizer = TfidfVectorizer(max_features = 1000)\n","tokenized_tweets = combined_df['tokenized_tweet'].tolist()\n","tfidf_matrix = tfidf_vectorizer.fit_transform(tokenized_tweets)\n","\n","# Convert TF-IDF matrix to DataFrame\n","X = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out(), index = combined_df.index)\n","linkage_matrix = linkage(X, method='ward')\n","\n","# Plot the dendrogram\n","plt.figure(figsize=(10, 6))\n","dendrogram(linkage_matrix, truncate_mode='level', p=7)\n","plt.title('Hierarchical Clustering of Tweets')\n","plt.xlabel('Samples')\n","plt.ylabel('Distance')\n","plt.show()"],"metadata":{"id":"LlsaPUhPoJS9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clustering = AgglomerativeClustering(n_clusters = 4, linkage='ward', compute_full_tree='auto')\n","y_predA = clustering.fit_predict(X)\n","\n","# Create a dictionary to store top terms for each cluster\n","top_terms_per_cluster = {}\n","\n","for cluster_id in range(4):\n","  # indices of data points in the current cluster\n","  cluster_indices = np.where(y_predA == cluster_id)[0]\n","\n","  # TF-IDF scores for data points in the current cluster\n","  cluster_tfidf = X.iloc[cluster_indices]\n","\n","  # Calculate mean TF-IDF scores for each term in the current cluster\n","  mean_tfidf_scores = cluster_tfidf.mean(axis=0)\n","\n","  # Select top 30 terms based on mean TF-IDF scores\n","  top_terms = mean_tfidf_scores.nlargest(30)\n","\n","  # Store top terms for the current cluster in the dictionary\n","  top_terms_per_cluster[cluster_id] = top_terms\n","\n","# Word clouds for each cluster\n","for cluster_id, terms in top_terms_per_cluster.items():\n","  wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(terms))\n","  plt.figure(figsize=(10, 5))\n","  plt.imshow(wordcloud, interpolation='bilinear')\n","  plt.title(f'Cluster {cluster_id} Word Cloud')\n","  plt.axis('off')\n","  plt.show()"],"metadata":{"id":"b1x0-YN5YgpV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate the percentage of pro, anti, and neutral tweets in each cluster\n","cluster_stance_counts = {}\n","\n","for cluster_id in range(4):\n","\n","    cluster_indices = np.where(y_predA == cluster_id)[0]\n","\n","    # Stance labels for data points in the current cluster\n","    cluster_stances = combined_df.iloc[cluster_indices]['label']\n","\n","    # Count the number of pro, anti, and neutral tweets in the current cluster\n","    pro_count = (cluster_stances == 1).sum()\n","    anti_count = (cluster_stances == -1).sum()\n","    neutral_count = (cluster_stances == 0).sum()\n","    total_count = len(cluster_indices)\n","\n","    # Calculate the percentage of pro, anti, and neutral tweets\n","    cluster_stance_counts[f'Cluster {cluster_id + 1}'] = {\n","        'Pro': round(pro_count / total_count * 100, 2),\n","        'Anti': round(anti_count / total_count * 100, 2),\n","        'Neutral': round(neutral_count / total_count * 100, 2)\n","    }\n","\n","# Display the stance distribution table for each cluster\n","stance_distribution_df = pd.DataFrame.from_dict(cluster_stance_counts, orient='index')\n","print(\"Stance Distribution per Cluster:\")\n","print(stance_distribution_df)\n"],"metadata":{"id":"8J8iJ6rAJBLU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Clustering Using Anti Tweets\n","\n"],"metadata":{"id":"P4QRdHKbpjJe"}},{"cell_type":"code","source":["anti_df = combined_df[combined_df['label'] == -1]\n","anti_tokenized_tweets = anti_df['tokenized_tweet'].tolist()\n","tfidf_vectorizer = TfidfVectorizer()\n","anti_tfidf_matrix = tfidf_vectorizer.fit_transform(anti_tokenized_tweets)\n","\n","# Convert TF-IDF matrix to DataFrame\n","anti_X = pd.DataFrame(anti_tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out(), index = anti_df.index)\n","linkage_matrix = linkage(anti_X, method='ward')\n","\n","# Plot the dendrogram\n","plt.figure(figsize=(10, 6))\n","dendrogram(linkage_matrix, truncate_mode='level', p=7)\n","plt.title('Hierarchical Clustering of Anti-Stance Tweets')\n","plt.xlabel('Samples')\n","plt.ylabel('Distance')\n","plt.show()"],"metadata":{"id":"z8q5iTLmphhk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clustering = AgglomerativeClustering(n_clusters = 12, linkage='ward')\n","anti_y_predA = clustering.fit_predict(anti_X)\n","\n","#Dictionary to store top terms for each cluster\n","top_terms_per_cluster = {}\n","for cluster_id in range(12):\n","  cluster_indices = np.where(anti_y_predA == cluster_id)[0]\n","  cluster_tfidf = anti_X.iloc[cluster_indices]\n","  mean_tfidf_scores = cluster_tfidf.mean(axis=0)\n","  top_terms = mean_tfidf_scores.nlargest(30)\n","  top_terms_per_cluster[cluster_id] = top_terms\n","\n","# Word clouds for each cluster\n","for cluster_id, terms in top_terms_per_cluster.items():\n","  wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(terms))\n","  plt.figure(figsize=(10, 5))\n","  plt.imshow(wordcloud, interpolation='bilinear')\n","  plt.title(f'Cluster {cluster_id} Word Cloud')\n","  plt.axis('off')\n","  plt.show()\n"],"metadata":{"id":"c8X1qQqzqcW8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Clustering Using Pro Tweets"],"metadata":{"id":"d6c0nstKpn6H"}},{"cell_type":"code","source":["pro_df = combined_df[combined_df['label'] == 1]\n","pro_tokenized_tweets = pro_df['tokenized_tweet'].tolist()\n","tfidf_vectorizer = TfidfVectorizer(max_features=800)\n","pro_tfidf_matrix = tfidf_vectorizer.fit_transform(pro_tokenized_tweets)\n","\n","# Convert TF-IDF matrix to DataFrame\n","pro_X = pd.DataFrame(pro_tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out(), index = pro_df.index)\n","linkage_matrix = linkage(pro_X, method='ward')\n","\n","# Plot the dendrogram\n","plt.figure(figsize=(10, 6))\n","dendrogram(linkage_matrix, truncate_mode='level', p=7)\n","plt.title('Hierarchical Clustering of Pro-Stance Tweets')\n","plt.xlabel('Samples')\n","plt.ylabel('Distance')\n","plt.show()"],"metadata":{"id":"oOY_0ByJpp8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clustering = AgglomerativeClustering(n_clusters = 6, linkage='ward')\n","pro_y_predA = clustering.fit_predict(pro_X)\n","\n","#Dictionary to store top terms for each cluster\n","top_terms_per_cluster = {}\n","\n","for cluster_id in range(5):\n","  cluster_indices = np.where(pro_y_predA == cluster_id)[0]\n","  cluster_tfidf = pro_X.iloc[cluster_indices]\n","  mean_tfidf_scores = cluster_tfidf.mean(axis=0)\n","  top_terms = mean_tfidf_scores.nlargest(30)\n","  top_terms_per_cluster[cluster_id] = top_terms\n","\n","# Word clouds for each cluster\n","for cluster_id, terms in top_terms_per_cluster.items():\n","  wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(terms))\n","  plt.figure(figsize=(10, 5))\n","  plt.imshow(wordcloud, interpolation='bilinear')\n","  plt.title(f'Cluster {cluster_id} Word Cloud')\n","  plt.axis('off')\n","  plt.show()"],"metadata":{"id":"7U4yr93qrJyy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Network Analysis"],"metadata":{"id":"UvuuNygWH8pD"}},{"cell_type":"markdown","source":["### K-Means Clusters for Network Graph"],"metadata":{"id":"0ZwGZHQEGMJZ"}},{"cell_type":"code","source":["combined_df = pd.read_csv('combined_data.csv')\n","combined_df.dropna(inplace=True)\n","\n","pro_df = combined_df[combined_df['label'] == 1]\n","pro_corpus = pro_df['tokenized_tweet'].tolist()\n","\n","anti_df = combined_df[combined_df['label'] == -1]\n","anti_corpus = anti_df['tokenized_tweet'].tolist()\n","\n","n_top_terms = 20\n","\n","\"\"\"\n","  Words like 'climate', 'change', 'global' and 'warming'\n","  occur commonly between both pro and anti tweets.\n","\"\"\"\n","\n","tfidf_vectorizer = TfidfVectorizer(stop_words=['english', 'climate', 'change', 'global' , 'warming'])\n","anti_X = tfidf_vectorizer.fit_transform(anti_corpus)\n","anti_vocab = tfidf_vectorizer.get_feature_names_out()\n","\n","anti_kmeans = KMeans(n_clusters=12, random_state=14, n_init='auto')\n","anti_clusters = anti_kmeans.fit_predict(anti_X)\n","anti_centroids = anti_kmeans.cluster_centers_\n","\n","top_term_indices = anti_centroids.argsort()[:, ::-1][:, :n_top_terms]\n","\n","top_terms_per_cluster_anti = {}\n","for cluster_id, indices in enumerate(top_term_indices):\n","  terms = [anti_vocab[idx] for idx in indices]\n","  top_terms_per_cluster_anti[f'Cluster {cluster_id + 1}'] = terms\n","\n","tfidf_vectorizer = TfidfVectorizer(stop_words=['english', 'climate', 'change', 'global' , 'warming'])\n","pro_X = tfidf_vectorizer.fit_transform(pro_corpus)\n","pro_vocab = tfidf_vectorizer.get_feature_names_out()\n","\n","pro_kmeans = KMeans(n_clusters=12, random_state=24, n_init='auto')\n","pro_clusters = pro_kmeans.fit_predict(pro_X)\n","pro_centroids = pro_kmeans.cluster_centers_\n","\n","top_term_indices = pro_centroids.argsort()[:, ::-1][:, :n_top_terms]\n","\n","top_terms_per_cluster_pro = {}\n","for cluster_id, indices in enumerate(top_term_indices):\n","  terms = [pro_vocab[idx] for idx in indices]\n","  top_terms_per_cluster_pro[f'Cluster {cluster_id + 1}'] = terms\n"],"metadata":{"id":"pORtZIacGLGU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Network Analysis"],"metadata":{"id":"A7N8WI9eGR8O"}},{"cell_type":"code","source":["# Initialize empty graph\n","G = nx.Graph()\n","\n","# Add nodes for each \"pro\" cluster with an attribute to indicate it's a \"pro\" cluster\n","for cluster_id, terms in top_terms_per_cluster_pro.items():\n","  G.add_node(f\"Pro {cluster_id}\", terms=terms, stance=\"pro\")\n","\n","# Add nodes for each \"anti\" cluster with an attribute to indicate it's an \"anti\" cluster\n","for cluster_id, terms in top_terms_per_cluster_anti.items():\n","  G.add_node(f\"Anti {cluster_id}\", terms=terms, stance=\"anti\")"],"metadata":{"id":"aJRABc1tH7iB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to calculate the intersection of top terms between two clusters\n","def calculate_shared_terms(cluster1_terms, cluster2_terms):\n","  return set(cluster1_terms).intersection(set(cluster2_terms))\n","\n","# Add edges between \"pro\" and \"anti\" clusters if they share a significant number of top terms\n","\n","SIGNIFICANT_THRESHOLD = 7\n","\n","for pro_id, pro_terms in top_terms_per_cluster_pro.items():\n","  for anti_id, anti_terms in top_terms_per_cluster_anti.items():\n","    shared_terms = calculate_shared_terms(pro_terms, anti_terms)\n","    if len(shared_terms) >= SIGNIFICANT_THRESHOLD:\n","      # Add an edge between the pro and anti cluster nodes with the number of shared terms as weight\n","      G.add_edge(f\"Pro {pro_id}\", f\"Anti {anti_id}\", weight=len(shared_terms), shared_terms=list(shared_terms))\n"],"metadata":{"id":"HPKQU95b9Hek"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(12, 12))\n","pos = nx.spring_layout(G, seed=42)  # For consistent layout between runs\n","\n","# Pro clusters are blue and Anti clusters are red\n","node_colors = [\"lightblue\" if G.nodes[node][\"stance\"] == \"pro\" else \"lightcoral\" for node in G]\n","nx.draw_networkx_nodes(G, pos, node_color=node_colors, alpha=0.9)\n","\n","# Draw edges with width proportional to the number of shared terms\n","edge_widths = [G[u][v]['weight'] / 2 for u, v in G.edges()]\n","nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.5)\n","\n","# Draw node labels\n","nx.draw_networkx_labels(G, pos)\n","\n","plt.title(\"Network of Pro and Anti Clusters Based on Shared Top Terms\")\n","plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"iMAzuZNp9VPb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate positions for each node using one of NetworkX's layout algorithms\n","pos = nx.spring_layout(G, seed=42)  # Seed for consistency\n","\n","# Data for nodes\n","node_x = []\n","node_y = []\n","node_hover_text = []  # Hover text information\n","node_color = []  # Color nodes differently based on their stance\n","\n","for node, node_attrs in G.nodes(data=True):\n","  x, y = pos[node]\n","  node_x.append(x)\n","  node_y.append(y)\n","  terms = ', '.join(node_attrs['terms'])\n","  node_hover_text.append(f\"{node}: {terms}\")\n","  node_color.append(\"blue\" if node_attrs['stance'] == \"pro\" else \"red\")\n","\n","# Data for edges\n","edge_x = []\n","edge_y = []\n","edge_hover_text = []  # Hover text for edges\n","\n","for edge in G.edges(data=True):\n","  x0, y0 = pos[edge[0]]  # Start position\n","  x1, y1 = pos[edge[1]]  # End position\n","  edge_x.extend([x0, x1, None])  # None creates a gap between edges\n","  edge_y.extend([y0, y1, None])\n","  shared_terms = ', '.join(edge[2]['shared_terms'])\n","  edge_hover_text.append(f\"Shared terms: {shared_terms}\")\n","\n","edge_midpoints = []\n","edge_hover_texts = []  # Store the hover texts for each edge\n","\n","for edge in G.edges(data=True):\n","  x0, y0 = pos[edge[0]]\n","  x1, y1 = pos[edge[1]]\n","  midpoint = ((x0 + x1) / 2, (y0 + y1) / 2)\n","  edge_midpoints.append(midpoint)\n","\n","  common_words = ', '.join(edge[2]['shared_terms'])\n","  edge_hover_texts.append(f\"Common words: {common_words}\")"],"metadata":{"id":"s7Wy1d6XDz1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create edge trace\n","edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=0.5, color='#888'), hoverinfo='text', mode='lines', text=edge_hover_text, name='Edges')\n","\n","# Create node trace\n","node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', hoverinfo='text', text=node_hover_text, marker=dict(color=node_color, size=10), name='Nodes')\n","\n","# Add midpoint nodes for edge hover info\n","midpoint_x, midpoint_y = zip(*edge_midpoints)  # Unpack the midpoints into x and y coordinates\n","\n","edge_info_trace = go.Scatter(x=midpoint_x, y=midpoint_y, mode='markers', hoverinfo='text', text=edge_hover_texts,\n","                             marker=dict(color='rgba(0,0,0,0)', size=5), name='Edge Info')\n","\n","fig = go.Figure(data=[edge_trace, node_trace, edge_info_trace],\n","                layout=go.Layout(\n","                  title='Network Graph of Pro and Anti Tweets',\n","                  titlefont_size=16,\n","                  showlegend=False,  # Only need to set this once\n","                  hovermode='closest',  # Only need to set this once\n","                  margin=dict(b=20, l=5, r=5, t=40),\n","                  annotations=[dict(\n","                    text=\"Hover over nodes and edges to see labels\",\n","                    showarrow=False, xref=\"paper\", yref=\"paper\",\n","                    x=0.005, y=-0.002)],\n","                  xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n","                  yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))\n","fig.show()"],"metadata":{"id":"sDyjhYfyEdqu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v_gbjJJp2Fc-"},"source":["# Git Operations\n","\n","This section is intended for internal use only, to facilitate code versioning and updates by the original author.\n","\n","If you're here to explore, or run the analysis parts of the notebook, please feel free to skip this section. Thank you!"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"MnkXINAbk_rK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712413887154,"user_tz":-240,"elapsed":1956,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}},"outputId":"bd1a61a9-05de-431e-f16f-7bde3db56974"},"outputs":[{"output_type":"stream","name":"stdout","text":["From https://git.cs.bham.ac.uk/projects-2023-24/rxs008\n"," * branch            main       -> FETCH_HEAD\n","Already up to date.\n"]}],"source":["!git pull origin main"]}],"metadata":{"colab":{"collapsed_sections":["ImjrvBUVv1R3","DkJYSSKNe-ug","l90eUXp9r9Bn","OLev5TPMe9Go","bkIIwlVB1nT3","FdDt_b82Wae7","ZERINy0BHDnt","z7ZAXrp9QGNE","CV9IAs-u9nAI","VRM85uW4rqy2","ZIPO_M-tthoM","P4QRdHKbpjJe","d6c0nstKpn6H","UvuuNygWH8pD","0ZwGZHQEGMJZ","A7N8WI9eGR8O","v_gbjJJp2Fc-"],"provenance":[],"authorship_tag":"ABX9TyMuFFDS5YdAI0VukjLxd0iw"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}