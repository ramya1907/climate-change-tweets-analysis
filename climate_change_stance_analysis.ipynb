{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNbLGauZ0n//IsetMS33OkJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Installation and Imports"],"metadata":{"id":"ennzsx_bJw5R"}},{"cell_type":"markdown","source":[],"metadata":{"id":"TlecZFlzJzpi"}},{"cell_type":"code","source":["import chardet\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import re\n","import unicodedata\n","from collections import Counter\n","import string\n","import nltk\n","import spacy\n","from PIL import Image\n","from wordcloud import WordCloud\n","\n","nlp = spacy.load('en_core_web_sm')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger') #pos_tag\n","\n","import en_core_web_sm\n","from collections import Counter\n","from nltk.corpus import stopwords, wordnet\n","from nltk.stem import WordNetLemmatizer\n","from nltk.probability import FreqDist\n","from nltk.tokenize import word_tokenize\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report, f1_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_score\n","from sklearn.decomposition import PCA\n","\n","from gensim.models import Word2Vec\n"],"metadata":{"id":"YL-lyixHzoSb","executionInfo":{"status":"ok","timestamp":1708613454897,"user_tz":-240,"elapsed":12857,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d4c9a9c1-fd51-43cd-d8da-ec01bb050eb5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","\n","# Choose the file from your local machine\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":109},"id":"ujl1FvrcOy8H","outputId":"d46779b7-d9c8-47fa-f8f0-6a37bf909f4b","executionInfo":{"status":"ok","timestamp":1708613559118,"user_tz":-240,"elapsed":104230,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-5525fd16-169c-42c4-81a4-6cf39679e44d\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-5525fd16-169c-42c4-81a4-6cf39679e44d\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving data_world.csv to data_world.csv\n","Saving kaggle.csv to kaggle.csv\n"]}]},{"cell_type":"markdown","source":["### Data Cleaning"],"metadata":{"id":"x4v575KTaJD6"}},{"cell_type":"code","source":["# Detect encoding\n","with open(\"data_world.csv\", 'rb') as f:\n","    result = chardet.detect(f.read())\n","    encoding = result['encoding']\n","\n","'''\n","  Initially, data_world.csv had 6091 rows.\n","  After skipping on bad lines, there are 6090 rows in this dataset\n","  Finally, after dropping duplicates, the number of rows is now 5541.\n","'''\n","\n","# Read CSV with detected encoding\n","data_world_df = pd.read_csv(\"data_world.csv\", encoding=encoding, on_bad_lines='skip')\n","\n","# Remove duplicate tweets\n","data_world_df = data_world_df.drop_duplicates(subset='tweet', keep='first')\n","\n","# Change representation of 'existence' values\n","data_world_df['existence'] = data_world_df['existence'].map({'Yes': 1, 'No': -1, 'N/A': 0})"],"metadata":{"id":"1Oa1IEPrnKyf","executionInfo":{"status":"ok","timestamp":1708613565314,"user_tz":-240,"elapsed":6200,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Detect encoding\n","with open(\"kaggle.csv\", 'rb') as f:\n","    result = chardet.detect(f.read())\n","    encoding = result['encoding']\n","\n","'''\n","  Initially, kaggle.csv had 43,943 rows.\n","  After dropping duplicates, the number of rows reduced to 41,033\n","  Finally, after removing rows labelled as \"News\", the dataset now has 31,960 rows\n","'''\n","# Repeat same steps for Kaggle dataset\n","kaggle_df = pd.read_csv(\"kaggle.csv\", encoding=encoding, on_bad_lines='skip')\n","kaggle_df = kaggle_df.drop_duplicates(subset='message', keep='first')\n","\n","# Remove rows where sentiment is labeled as News (value = 2)\n","kaggle_df = kaggle_df.loc[kaggle_df['sentiment'] != 2]"],"metadata":{"id":"mO3yR7Exxtm6","executionInfo":{"status":"ok","timestamp":1708613591493,"user_tz":-240,"elapsed":26183,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["df_selected_columns = data_world_df[['tweet', 'existence']].rename(columns={'tweet': 'og_tweet','existence': 'label'})\n","df_kaggle_selected_columns = kaggle_df[['message', 'sentiment']].rename(columns={'message': 'og_tweet', 'sentiment': 'label'})\n","\n","# Concatenate the two DataFrames vertically\n","df = pd.concat([df_selected_columns, df_kaggle_selected_columns], ignore_index=True)\n","\n","#df.label.value_counts()\n","#df.info()\n","\n","# 5066 rows with missing labels\n","missing_label_rows = df[df['label'].isnull()]\n","df.dropna(subset=['label'], inplace=True)\n","\n","# Now the number of rows for both columns are equal\n","#df.info()\n","#round(df['label'].value_counts()*100/len(df),2)"],"metadata":{"id":"JC1R_j2s1ZQz","executionInfo":{"status":"ok","timestamp":1708613591494,"user_tz":-240,"elapsed":33,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["'''\n","Extracting hashtags from dataset for visualization later\n","before tweets undergo pre-processing\n","'''\n","\n","def extract_top_hashtags(tweets):\n","\n","  hashtags_list = []\n","\n","  for tweet in tweets:\n","      hashtags = re.findall(r\"#(\\w+)\", tweet)\n","      hashtags_list.extend(hashtags)\n","\n","  frequency = nltk.FreqDist(hashtags_list)\n","\n","  hashtag_df = pd.DataFrame({'hashtag': list(frequency.keys()), 'count': list(frequency.values())})\n","\n","  # Select the top 15 hashtags\n","  hashtag_df = hashtag_df.nlargest(15, columns=\"count\")\n","\n","  return hashtag_df\n","\n","# Extracting the hashtags from tweets in each class\n","pro_hashtags = extract_top_hashtags(df['og_tweet'][df['label'] == 1])\n","anti_hashtags = extract_top_hashtags(df['og_tweet'][df['label'] == -1])\n","neutral_hashtags = extract_top_hashtags(df['og_tweet'][df['label'] == 0])\n"],"metadata":{"id":"HmOJw1x3KYzC","executionInfo":{"status":"ok","timestamp":1708613591496,"user_tz":-240,"elapsed":34,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["'''\n","Initializing a dataframe to store tweets awaiting classification labels\n","based on the selected model.\n","'''\n","test_df = df['og_tweet'].sample(n=7500, random_state=42)\n","\n","# Remove selected tweets from the DataFrame\n","df = df.drop(test_df.index)\n","test_df = test_df.reset_index(drop=True).to_frame(name='og_tweet')"],"metadata":{"id":"bEEIi0HtPQsN","executionInfo":{"status":"ok","timestamp":1708613591496,"user_tz":-240,"elapsed":33,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### Data Pre-Processing"],"metadata":{"id":"jPHBm-B4cQ-z"}},{"cell_type":"code","source":["# Function to fix encoding issues\n","def fix_encoding(text):\n","    try:\n","      decoded_text = text.encode('utf-8').decode('utf-8')\n","    except Exception as e:  # Catch other potential errors\n","      decoded_text = ''  # Return empty string for other errors\n","    return decoded_text\n","\n","def remove_punctuation(text):\n","    cleaned_text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n","    return cleaned_text\n"],"metadata":{"id":"oX-acGRVWWe1","executionInfo":{"status":"ok","timestamp":1708613591496,"user_tz":-240,"elapsed":32,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Method for Pre-processing\n","def clean_tweet(tweet):\n","\n","  # Remove retweet texts \"RT\"\n","  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","\n","  # Converting to lower case\n","  tweet = tweet.lower()\n","\n","  # Remove URLs\n","  tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n","\n","  # Remove placeholders saying [link] which was done by prior text cleaning\n","  tweet = re.sub(r'\\[link\\]', '', tweet)\n","\n","  # Remove numbers\n","  tweet = re.sub(r'\\d+', '', tweet)\n","\n","  # Remove mentions\n","  tweet = re.sub('@[\\w]*', '', tweet)\n","\n","  # Remove punctuation\n","  tweet = remove_punctuation(tweet)\n","\n","  # Remove diamond symbol\n","  tweet = re.sub(r\"U+FFFD \", '', tweet)\n","\n","  # Remove extra whitespace\n","  tweet = re.sub(r'\\s\\s+', '', tweet)\n","\n","  # Remove leading spaces\n","  tweet = tweet.lstrip(' ')\n","\n","  tweet = unicodedata.normalize('NFKD', tweet).encode('ascii','ignore').decode('utf-8', 'ignore')\n","\n","  # Fix encoding issues\n","  tweet = fix_encoding(tweet)\n","\n","  return tweet\n"],"metadata":{"id":"dxqwgZHocQYT","executionInfo":{"status":"ok","timestamp":1708614407020,"user_tz":-240,"elapsed":613,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"sDebeT_3GIoQ"}},{"cell_type":"code","source":["# Perform Pre-Processing on both DataFrames\n","df['cleaned_tweet'] = df['og_tweet'].apply(clean_tweet)\n","test_df['processed_tweet'] = test_df['og_tweet'].apply(clean_tweet)\n"],"metadata":{"id":"fBjkXtcR-6Yn","executionInfo":{"status":"ok","timestamp":1708614414417,"user_tz":-240,"elapsed":1102,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h01mVJCVk8o5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenizing\n","nlp = spacy.load('en_core_web_sm')\n","df['tokenized_tweet'] = df['cleaned_tweet'].apply(lambda x: nlp(x))\n","test_df['processed_tweet'] = test_df['processed_tweet'].apply(lambda x: nlp(x))\n","\n","stop_words = stopwords.words('english')\n","stop_words += list(string.punctuation) # !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n","stop_words += list(string.ascii_lowercase) # letters from 'a' to 'z'\n","\n","def clean_tokens(tokens):\n","  cleaned_tokens = [word.text for word in tokens if word.text not in set(stop_words)]\n","  return cleaned_tokens\n","\n","df['tokenized_tweet'] = df['tokenized_tweet'].apply(clean_tokens)\n","test_df['processed_tweet'] = test_df['processed_tweet'].apply(clean_tokens)\n"],"metadata":{"id":"zjXXrOWIe6rQ","executionInfo":{"status":"ok","timestamp":1708614663944,"user_tz":-240,"elapsed":245519,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Removing rare words from dataset to reduce bias\n","all_tweets = [' '.join(tokens) for tokens in df['tokenized_tweet']]\n","\n","def find_rare_words(threshold=2, all_text=all_tweets, top_n_rare_words=10):\n","  words = nltk.word_tokenize(' '.join(all_text))\n","  word_freq = Counter(words)\n","\n","  rare_words = [word for word, freq in word_freq.items() if freq < threshold][:top_n_rare_words]\n","\n","  return rare_words\n","\n","def remove_rare_words(text, rare_words):\n","  filtered_words = [word for word in text if word not in rare_words]\n","  return filtered_words\n","\n","rare_words_list = find_rare_words()\n","df['tokenized_tweet'] = df['tokenized_tweet'].apply(remove_rare_words, rare_words=rare_words_list)\n","test_df['tokenized_tweet'] = test_df['processed_tweet'].apply(remove_rare_words, rare_words=rare_words_list)\n"],"metadata":{"id":"psWtjx5EoZ7H","executionInfo":{"status":"ok","timestamp":1708614664964,"user_tz":-240,"elapsed":1024,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Lemmatizing\n","def lemma(df):\n","\n","  # Add part-of-speech tags to the 'tweet' column\n","  df['pos_tags'] = df['tokenized_tweet'].apply(nltk.tag.pos_tag)\n","\n","  def get_wordnet_pos(tweet_tag):\n","\n","    # Map Penn Treebank POS tags to WordNet POS tags.\n","    pos_mapping = {\n","        'J': wordnet.ADJ,\n","        'V': wordnet.VERB,\n","        'N': wordnet.NOUN,\n","        'R': wordnet.ADV\n","    }\n","    return wordnet.NOUN\n","\n","  lemmatizer = WordNetLemmatizer()\n","\n","  # Lemmatize the 'tweet' column based on part-of-speech tags\n","  df['lemma'] = df['pos_tags'].apply(lambda x: [lemmatizer.lemmatize(word, get_wordnet_pos(pos_tag)) for word, pos_tag in x])\n","\n","  # ['apple', 'banana', 'orange'] -> 'apple banana orange'\n","  df['lemma'] = [' '.join(map(str, l)) for l in df['lemma']]\n","\n","  df.drop('pos_tags', axis=1, inplace=True)\n","\n","  return df\n","\n","df = lemma(df)"],"metadata":{"id":"G-3bguL-xqcz","executionInfo":{"status":"ok","timestamp":1708614687412,"user_tz":-240,"elapsed":22453,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Lemmatizing unlabelled dataset\n","def test_lemma(test_df):\n","\n","  # Add part-of-speech tags to the 'tweet' column\n","  test_df['pos_tags'] = test_df['tokenized_tweet'].apply(nltk.tag.pos_tag)\n","\n","  def get_wordnet_pos(tweet_tag):\n","\n","    # Map Penn Treebank POS tags to WordNet POS tags.\n","    pos_mapping = {\n","        'J': wordnet.ADJ,\n","        'V': wordnet.VERB,\n","        'N': wordnet.NOUN,\n","        'R': wordnet.ADV\n","    }\n","    return wordnet.NOUN\n","\n","  lemmatizer = WordNetLemmatizer()\n","\n","  # Lemmatize the 'tweet' column based on part-of-speech tags\n","  test_df['lemma'] = test_df['pos_tags'].apply(lambda x: [lemmatizer.lemmatize(word, get_wordnet_pos(pos_tag)) for word, pos_tag in x])\n","\n","  # ['apple', 'banana', 'orange'] -> 'apple banana orange'\n","  test_df['lemma'] = [' '.join(map(str, l)) for l in test_df['lemma']]\n","\n","  test_df.drop('pos_tags', axis=1, inplace=True)\n","\n","  return test_df\n","\n","test_df = test_lemma(test_df)"],"metadata":{"id":"6z4wV6wZWrGb","executionInfo":{"status":"ok","timestamp":1708614693140,"user_tz":-240,"elapsed":5732,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["### Visualisations"],"metadata":{"id":"l90eUXp9r9Bn"}},{"cell_type":"code","source":["# Finding the 25 most commonly mentioned words for each sentiment label\n","def calculate_word_frequency(text_corpus):\n","\n","    count_vectorizer = CountVectorizer(stop_words='english')\n","    word_matrix = count_vectorizer.fit_transform(text_corpus)\n","\n","    # Calculate the sum of words in the documents and determine the frequency of each word\n","    sum_words = word_matrix.sum(axis=0)\n","    word_freq = [(word, sum_words[0, i]) for word, i in count_vectorizer.vocabulary_.items()]\n","\n","    # Sort the data based on frequencies in descending order\n","    sorted_word_frequencies = sorted(word_freq, key=lambda x: x[1], reverse=True)\n","\n","    top_words_df = pd.DataFrame(word_freq, columns=['word', 'frequency'])\n","\n","    # Select the words with highest frequencies\n","    top_words_df = top_words_df.head(25)\n","\n","    return top_words_df\n","\n","# Retrieving the top 25 words in each class\n","pro_top_25 = calculate_word_frequency(df['lemma'][df['label']==1])\n","anti_top_25 = calculate_word_frequency(df['lemma'][df['label']==-1])\n","neutral_top_25 = calculate_word_frequency(df['lemma'][df['label']==0])"],"metadata":{"id":"g7vMnE6ODhPb","executionInfo":{"status":"aborted","timestamp":1708614389337,"user_tz":-240,"elapsed":23,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Turning the above variables into lists to use as data for wordclouds\n","pro_list = ' '.join([text for text in pro_top_25['word']])\n","anti_list = ' '.join([text for text in anti_top_25['word']])\n","neutral_list = ' '.join([text for text in neutral_top_25['word']])\n","\n","# Generating wordclouds\n","anti_wc = WordCloud(background_color='white', colormap='gist_heat', width = 800, height = 500).generate(anti_list)\n","pro_wc = WordCloud(background_color='white', colormap='Greens', width = 800, height = 500).generate(pro_list)\n","neutral_wc = WordCloud(background_color='white', colormap='PuBuGn_r', width = 800, height = 500).generate(neutral_list)\n","\n","fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","# Plot Anti Word Cloud\n","axes[0].imshow(anti_wc, interpolation='bilinear')\n","axes[0].set_title('Anti Word Cloud')\n","axes[0].axis('off')\n","\n","# Plot Pro Word Cloud\n","axes[1].imshow(pro_wc, interpolation='bilinear')\n","axes[1].set_title('Pro Word Cloud')\n","axes[1].axis('off')\n","\n","# Plot Neutral Word Cloud\n","axes[2].imshow(neutral_wc, interpolation='bilinear')\n","axes[2].set_title('Neutral Word Cloud')\n","axes[2].axis('off')\n","\n","# Show the plot\n","plt.show()"],"metadata":{"id":"bIRSh5KtPr92","executionInfo":{"status":"aborted","timestamp":1708614389337,"user_tz":-240,"elapsed":23,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hashtag frequency visualisation\n","fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n","plt.yscale('log')\n","\n","# Plot top hashtags for each sentiment category\n","pro_hashtags.set_index('hashtag').plot(kind='bar', ax=axes[0], color='green', title='Top Pro-Hashtages')\n","axes[0].set_ylabel('Count')\n","\n","anti_hashtags.set_index('hashtag').plot(kind='bar', ax=axes[1], color='orangered', title='Top Anti-Hashtags')\n","axes[1].set_ylabel('Count')\n","\n","neutral_hashtags.set_index('hashtag').plot(kind='bar', ax=axes[2], color='navy', title='Top Neutral-Hashtags')\n","axes[2].set_ylabel('Count')\n","\n","plt.tight_layout()\n","plt.show()\n","\n"],"metadata":{"id":"zNJ3bSzjC9V-","executionInfo":{"status":"aborted","timestamp":1708614389337,"user_tz":-240,"elapsed":20,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sentiment Analysis: Naive Bayes"],"metadata":{"id":"FdDt_b82Wae7"}},{"cell_type":"code","source":["# Separate dataset into feature and target variables\n","x = df['lemma']\n","y = df['label']\n","\n","# Dividing data into training and test sets\n","x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.25, random_state=42)\n","\n","# Vectorize text to numbers\n","vec = CountVectorizer(stop_words='english')\n","x_train = vec.fit_transform(x_train).toarray()\n","x_test = vec.transform(x_test).toarray()\n","\n","# Naive Bayes Approach\n","nb = MultinomialNB()\n","nb.fit(x_train, y_train)\n","nb.score(x_test, y_test)\n","\n","nb_pred = nb.predict(x_test)\n","accuracy = accuracy_score(y_test, nb_pred)\n","f1 = f1_score(y_test, nb_pred, average=\"weighted\")\n","print(classification_report(y_test, nb_pred))\n"],"metadata":{"id":"EUO-6QOsWZrA","executionInfo":{"status":"aborted","timestamp":1708614389337,"user_tz":-240,"elapsed":20,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"USheg8j_HOlM"}},{"cell_type":"code","source":["# Predict sentiment for test_df using Naive Bayes\n","X_test = vec.transform(test_df['lemma']).toarray()\n","test_df['nb_sentiment'] = nb.predict(X_test)\n"],"metadata":{"id":"0r5UT8Vrrz6b","executionInfo":{"status":"aborted","timestamp":1708614389337,"user_tz":-240,"elapsed":19,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"HW3bFRF7dm_V"}},{"cell_type":"markdown","source":["### K-Means"],"metadata":{"id":"QtkWw52cUbvY"}},{"cell_type":"code","source":["# Function to train Word2Vec model and perform Word2Vec embedding\n","def perform_word2vec_embedding(tokenized_tweets):\n","  word2vec_model = Word2Vec(sentences=tokenized_tweets, vector_size=100, window=5, min_count=1, workers=4)\n","  word_embeddings = word2vec_model.wv\n","  return word_embeddings\n","\n","# Function to aggregate word vectors for each document\n","def aggregate_vectors(tokens, word_embeddings):\n","  vectors = [word_embeddings[word] for word in tokens if word in word_embeddings]\n","  if vectors:\n","    return np.mean(vectors, axis=0)\n","  else:\n","    return np.zeros(word_embeddings.vector_size)\n","\n","# Function to perform K-means clustering\n","def perform_kmeans_clustering(X, k):\n","  kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n","  clusters = kmeans.fit_predict(X)\n","  wcss = kmeans.inertia_\n","  return kmeans, clusters, wcss\n","\n","# Function to plot the elbow curve\n","def plot_elbow_curve(wcss_values, k_range, sentiment):\n","  plt.plot(k_range, wcss_values, marker='o')\n","  plt.title(f'Elbow Method for Optimal K ({sentiment} Sentiment)')\n","  plt.xlabel('Number of Clusters (K)')\n","  plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n","  plt.xticks(k_range)\n","  plt.show()\n","\n","# Function to print top terms per cluster\n","def generate_top_terms_df(X, cluster_assignments, feature_names, n_top_terms=10):\n","  top_terms_dict = {}\n","  for cluster_id in range(len(set(cluster_assignments))):\n","    cluster_indices = np.where(cluster_assignments == cluster_id)[0]  # Indices of data points in the cluster\n","\n","    if len(cluster_indices) > 0:\n","      cluster_X = [X[i] for i in cluster_indices]\n","      cluster_term_freq = np.sum(cluster_X, axis=0)\n","      top_term_indices = np.argsort(cluster_term_freq)[::-1][:n_top_terms]\n","      top_terms = [feature_names[i] for i in top_term_indices]\n","      top_terms_dict[f'Cluster {cluster_id + 1}'] = top_terms\n","    else:\n","      top_terms_dict[f'Cluster {cluster_id + 1}'] = []\n","\n","  top_terms_df = pd.DataFrame(top_terms_dict)\n","  return top_terms_df\n"],"metadata":{"id":"Vu7LKW-CZphH","executionInfo":{"status":"ok","timestamp":1708617772503,"user_tz":-240,"elapsed":2,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["# Create Pro and Anti DataFrames\n","pro_df = df[df['label'] == 1]\n","anti_df = df[df['label'] == -1]\n","\n","pro_tokenized_tweets = pro_df['tokenized_tweet'].tolist()\n","anti_tokenized_tweets = anti_df['tokenized_tweet'].tolist()\n","\n","# Apply Word2Vec embedding for pro and anti sentiments\n","pro_word_embeddings = perform_word2vec_embedding(pro_tokenized_tweets)\n","pro_X = [aggregate_vectors(tokens, pro_word_embeddings) for tokens in pro_tokenized_tweets]\n","\n","anti_word_embeddings = perform_word2vec_embedding(anti_tokenized_tweets)\n","anti_X = [aggregate_vectors(tokens, anti_word_embeddings) for tokens in anti_tokenized_tweets]\n","\n","# Perform PCA for pro and anti tweets\n","pca_pro = PCA(n_components=2)\n","pro_X_2d = pca_pro.fit_transform(pro_X)\n","\n","pca_anti = PCA(n_components=2)\n","anti_X_2d = pca_anti.fit_transform(anti_X)\n"],"metadata":{"id":"pFdOyixtbtro","executionInfo":{"status":"ok","timestamp":1708617698533,"user_tz":-240,"elapsed":4322,"user":{"displayName":"Ramya Srinivasan","userId":"00752891766736604334"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["# Define the range of k values\n","k_range = range(2, 11)\n","\n","# Perform K-means clustering for pro sentiment\n","pro_wcss_values = []\n","for k in k_range:\n","    _, _, wcss = perform_kmeans_clustering(pro_X, k)\n","    pro_wcss_values.append(wcss)\n","\n","plot_elbow_curve(pro_wcss_values, k_range, 'Pro')\n","\n","# Perform K-means clustering for anti sentiment\n","anti_wcss_values = []\n","for k in k_range:\n","    _, _, wcss = perform_kmeans_clustering(anti_X, k)\n","    anti_wcss_values.append(wcss)\n","\n","plot_elbow_curve(anti_wcss_values, k_range, 'Anti')\n"],"metadata":{"id":"UWQizFwYf8bH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pro_kmeans, pro_clusters, pro_wcss = perform_kmeans_clustering(pro_X, k=5)\n","pro_top_terms_df = generate_top_terms_df(pro_X, pro_clusters, list(pro_word_embeddings.key_to_index.keys()))\n","print(pro_top_terms_df)\n","\n","anti_kmeans, anti_clusters, anti_wcss = perform_kmeans_clustering(anti_X, k=5)\n","anti_top_terms_df = generate_top_terms_df(anti_X, anti_clusters, list(anti_word_embeddings.key_to_index.keys()))\n","print(anti_top_terms_df)"],"metadata":{"id":"2scBZ-VZg2Is"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define colors for each cluster\n","colors_pro = ['red', 'blue', 'green', 'orange', 'purple']\n","colors_anti = ['cyan', 'magenta', 'yellow', 'black', 'gray']\n","\n","# Plot for pro sentiment\n","plt.figure(figsize=(10, 8))\n","for i in range(5):\n","    plt.scatter(pro_X_2d[pro_clusters == i, 0], pro_X_2d[pro_clusters == i, 1], color=colors_pro[i], label=f'Cluster {i+1}', alpha=0.5)\n","plt.title('PCA Visualization with K-means Clustering for Pro Sentiment')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.legend()\n","plt.show()\n","\n","# Plot for anti sentiment\n","plt.figure(figsize=(10, 8))\n","for i in range(5):\n","    plt.scatter(anti_X_2d[anti_clusters == i, 0], anti_X_2d[anti_clusters == i, 1], color=colors_anti[i], label=f'Cluster {i+1}', alpha=0.5)\n","plt.title('PCA Visualization with K-means Clustering for Anti Sentiment')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"dwlRNDTYiH2W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git pull origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MnkXINAbk_rK","outputId":"578dcdc3-d18f-44d6-8778-778768b82b1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["From https://git.cs.bham.ac.uk/projects-2023-24/rxs008\n"," * branch            main       -> FETCH_HEAD\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"VRM85uW4rqy2"}}]}